{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41983c13-4b64-424a-a6a9-65861d0de545",
   "metadata": {},
   "source": [
    "### Get Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44608666-eab7-49f6-a789-481ac819bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    " \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017298c6-8e6f-4192-9fd3-7bf31947dd24",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403ed43c-f662-4c1d-9cf0-598a814300a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    " \n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    " \n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    " \n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
    " \n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    " \n",
    "df.churn = (df.churn == 'yes').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec99304c-38e5-45c8-b291-a54cdede59f5",
   "metadata": {},
   "source": [
    "- Again we use the train_test_split function to divide the dataset in full_train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9c50a49-e845-45f8-9c08-0040eac68250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "# Data splitting\n",
    " \n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9a231-0725-44fb-b9c3-a4ce3d6dfdac",
   "metadata": {},
   "source": [
    "the numerical column names and what are the categorical column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ed77381-717c-4089-9bd6-6a0a85635e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
    " \n",
    "categorical = ['gender', 'seniorcitizen', 'partner', 'dependents',\n",
    "       'phoneservice', 'multiplelines', 'internetservice',\n",
    "       'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport',\n",
    "       'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling',\n",
    "       'paymentmethod']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab10a17-be5b-43a6-95d2-907db1cdee14",
   "metadata": {},
   "source": [
    "### Following is Train Function.\n",
    "It has three arguments- the training dataframe and target values y_train, and the third argument  is C\n",
    " Logistic Regression Parameter for our model.\n",
    " First step here is to create dictionaries from the categorical columns, remember the numerical columns are ignored here. Next we create a DictVectorizer instance which we need to use fit_transform function on the dictionaries. So we get the X_train. Then we create our model which is a logistic regression model, that we can use for training (fit function) based on the training data (X_train and y_train). To apply the model later we need to return the DictVectorizer and the model as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b23bbf-9e90-49f9-a897-9168a4d9125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    " \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    " \n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    " \n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670797f-5f2d-45df-8a17-31b0fec81851",
   "metadata": {},
   "source": [
    "As I just mentioned in the paragraph before to use the model we need also the DictVectorizer. Both are arguments for the predict function which is show in the next snippet. Besides both arguments you also need a dataframe where we can provide a prediction for. First step here is the same like in training function, we need to get the dictionaries. This can be transformed by the DictVectorizer so we get the X, what we need to make a prediction on. What we return here is the predicted probability for churning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b217e5-ad9e-4780-8d59-b31652a1e8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, dv, model):\n",
    "     dicts = df[categorical + numerical].to_dict(orient='records')\n",
    " \n",
    "     X = dv.transform(dicts)\n",
    "     y_pred = model.predict_proba(X)[:,1]\n",
    " \n",
    "     return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f922ca-8028-46e0-b829-a6d1dbb7e294",
   "metadata": {},
   "source": [
    "The first one is the C value for the Logistic Regression model, and the ‘n_splits’ parameter tells us how many splits we’re going to use in K-Fold cross-validation. Here, we’re using 5 splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ed0cfa-098a-44c0-9f60-d9483fb585b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1.0\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6381f4e2-caad-4b9b-a4bd-39d60de41f76",
   "metadata": {},
   "source": [
    "implemented K-Fold cross validation, where we use the parameters from the last snippet. The for loop loops over all folds and does a training for each. After that we calculate the roc_auc_score and collect the values for each fold. At the end the mean score and the standard deviation for all folds are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac61db03-612c-4414-8b61-1f0ffabbf979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AJAY\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\AJAY\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\AJAY\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1.0 0.842 +- 0.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AJAY\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1) \n",
    "scores = []\n",
    " \n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    " \n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    " \n",
    "    dv, model = train(df_train, y_train, C=C)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    " \n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    " \n",
    "print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))\n",
    " \n",
    "# Output: C=1.0 0.841 +- 0.008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d919b-a96a-4d19-b397-e0207c0997b8",
   "metadata": {},
   "source": [
    "- The last snipped doesn’t show the score for each fold seperately but here you can see each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3618d16d-cd49-44c0-a108-e2d2375424c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8446554309174117,\n",
       " 0.8451136438882267,\n",
       " 0.833257074051776,\n",
       " 0.8347529097653001,\n",
       " 0.8519069059646702]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb18864-dbe8-444b-9fb7-7f2e8e4078b6",
   "metadata": {},
   "source": [
    "- Last step is to train the final model based on the full_train data.\n",
    "-  The steps here are similar to the steps mentioned before. First is model training, then predicting the test data, and lastly calculate the roc_auc_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac40b572-d9e2-420b-a129-eb427d7fcf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AJAY\\miniconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8583138331870823"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
    "y_pred = predict(df_test, dv, model)\n",
    "y_test = df_test.churn.values\n",
    " \n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05552524-058e-44c6-9156-99fadb026c61",
   "metadata": {},
   "source": [
    "### For saving the model we’ll use pickle, what is a built in library for saving Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "777654c4-df82-42f3-8b30-47e5b2a2e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e118bc-80e3-4c44-b313-e830b2d3ea42",
   "metadata": {},
   "source": [
    "- First, we need to name our model file before we can write it to a file.\n",
    "- The following snippet demonstrates two ways of naming the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "013f0e7d-c4cf-4bb2-a018-787a06dcf1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_C=1.0.bin'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file = 'model_C=%s.bin' % C\n",
    "output_file\n",
    "# Output: 'model_C=1.0.bin'\n",
    " \n",
    "output_file = f'model_C={C}.bin'\n",
    "output_file\n",
    "# Output: 'model_C=1.0.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ba787-49bd-458f-98f7-76703a983d50",
   "metadata": {},
   "source": [
    "- Now we want to create a file with that file name. ‘wb’ means Write Binary.\n",
    "-  We need to save DictVectorizer and the model as well, because with just the model we’ll not be able to translate a customer into a feature matrix. Closing the file is crucial. Otherwise, we cannot be certain whether this file truly contains the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19bb151b-f386-40d1-b9f9-f82c0e0872d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = open(output_file, 'wb')\n",
    " \n",
    "pickle.dump((dv, model), f_out)\n",
    " \n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d8aeb-5185-47ea-af1d-9e4b5b9318a8",
   "metadata": {},
   "source": [
    "To avoid accidentally forgetting to close the file, we can use the ‘with’ statement, which ensures that the file is closed automatically. Everything we do inside the ‘with’ statement keeps the file open. However, once we exit this statement, the file is automatically closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffac44d-2c65-4a76-a3c7-61605ca0a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open(output_file, 'wb') as f_out:\n",
    "#     pickle.dump((dv, model), f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad32462d-ebb5-41da-b001-194126450b69",
   "metadata": {},
   "source": [
    "### Loading the Model with Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c372a34-d9d9-4d77-8628-ab8fd300249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "import pickle\n",
    "\t\n",
    "model_file = 'model_C=1.0.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676347fd-85ee-47d5-8ec5-07ef3fdf41a6",
   "metadata": {},
   "source": [
    "- We also utilize the ‘with’ statement for loading the model.\n",
    "-  Here, ‘rb’ denotes Read Binary. We employ the ‘load’ function from pickle, which returns both the DictVectorizer and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26c9602c-79ee-4c4c-84cb-1bdc2ee35412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DictVectorizer(sparse=False), LogisticRegression(max_iter=1000))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\t\n",
    "with open(model_file, 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)\n",
    " \n",
    "dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b889943-ddde-4c66-845f-3e3a5135203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = {\n",
    "    'gender': 'female',\n",
    "    'seniorcitizen': 0,\n",
    "    'partner': 'yes',\n",
    "    'dependents': 'no',\n",
    "    'phoneservice': 'no',\n",
    "    'multiplelines': 'no_phone_service',\n",
    "    'internetservice': 'dsl',\n",
    "    'onlinesecurity': 'no',\n",
    "    'onlinebackup': 'yes',\n",
    "    'deviceprotection': 'no',\n",
    "    'techsupport': 'no',\n",
    "    'streamingtv': 'no',\n",
    "    'streamingmovies': 'no',\n",
    "    'contract': 'month-to-month',\n",
    "    'paperlessbilling': 'yes',\n",
    "    'paymentmethod': 'electronic_check',\n",
    "    'tenure': 1,\n",
    "    'monthlycharges': 29.85,\n",
    "    'totalcharges': 29.85\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43977319-9011-43d0-9060-a793ee047e05",
   "metadata": {},
   "source": [
    "- Before we can apply the predict function to this customer we need to turn it into a feature matrix.\n",
    "-  The DictVectorizer expects a list of dictionaries, that’s why we create a list with one customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b4e005d-35d4-49ad-bec5-6ba5ca49a6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n",
       "         0.  ,  1.  ,  0.  ,  0.  , 29.85,  0.  ,  1.  ,  0.  ,  0.  ,\n",
       "         0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,\n",
       "         0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,\n",
       "         0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  , 29.85]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dv.transform([customer])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff6ac6-2394-4bef-bd8b-721c9219ee5d",
   "metadata": {},
   "source": [
    "We use predict function to get the probability that this particular customer is going to churn. We’re interested in the second element, so we need to set the row=0 and column=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42e557c9-995c-415e-8cde-747e0e77ad1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6282324545713467)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X)\n",
    "model.predict_proba(X)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5fc9f-ec4a-46d2-bdb1-88c186fb4165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
